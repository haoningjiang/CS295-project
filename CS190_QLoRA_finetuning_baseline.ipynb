{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8diAZHUjfzR"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/artidoro/qlora.git\n",
        "!pip install -r qlora/requirements.txt\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use huggingface account details here\n",
        "%env USER=\n",
        "%env TOKEN="
      ],
      "metadata": {
        "id": "BWOUhuc5l1a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://$USER:$TOKEN@huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "\n",
        "!wget --header=\"Authorization: Bearer ${TOKEN}\" https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/model-00001-of-00004.safetensors -O Meta-Llama-3-8B/model-00001-of-00004.safetensors\n",
        "!wget --header=\"Authorization: Bearer ${TOKEN}\" https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/model-00002-of-00004.safetensors -O Meta-Llama-3-8B/model-00002-of-00004.safetensors\n",
        "!wget --header=\"Authorization: Bearer ${TOKEN}\" https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/model-00003-of-00004.safetensors -O Meta-Llama-3-8B/model-00003-of-00004.safetensors\n",
        "!wget --header=\"Authorization: Bearer ${TOKEN}\" https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/model-00004-of-00004.safetensors -O Meta-Llama-3-8B/model-00004-of-00004.safetensors"
      ],
      "metadata": {
        "id": "UB5jqIUCkK31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick an acceptable max_steps value for experiments. 100 is set by default and is reasonable for a 2 hour example experiment.\n",
        "!python3 qlora/qlora.py --model_name_or_path Meta-Llama-3-8B/ --max_steps 100"
      ],
      "metadata": {
        "id": "SKb3DM9sxVqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both models can't be loaded in simultaneously due to Colab memory constraints. Uncomment the original model and comment out the finetuned model if you wish to prompt the original Llama3."
      ],
      "metadata": {
        "id": "ZNPkQ839dCi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel\n",
        "\n",
        "MODEL_DIR = \"Meta-Llama-3-8B\"  # base model\n",
        "ADAPTER_PATH = \"output/checkpoint-10/adapter_model\"  # adapter weights\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "#original_model = LlamaForCausalLM.from_pretrained(\n",
        "#    MODEL_DIR,\n",
        "#    return_dict=True,\n",
        "#    quantization_config=quantization_config,\n",
        "#    torch_dtype=torch.float16\n",
        "#)\n",
        "\n",
        "finetuned_model = LlamaForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    return_dict=True,\n",
        "   quantization_config=quantization_config,\n",
        "   torch_dtype=torch.float16\n",
        ")\n",
        "finetuned_model = PeftModel.from_pretrained(finetuned_model, ADAPTER_PATH, offload_folder=\"/content/sample_data\")\n",
        "finetuned_model.eval()\n",
        "\n",
        "config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=20,\n",
        "    top_p=1.0,\n",
        ")\n",
        "\n",
        "task = \"text-generation\"\n",
        "#original_pipe = pipeline(task, model=original_model, tokenizer=tokenizer)\n",
        "finetuned_pipe = pipeline(task, model=finetuned_model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "bblmEfkX8ryY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\")\n",
        "prompt = input()\n",
        "\n",
        "#original_output = original_pipe(prompt)[0]['generated_text'].split(prompt, 1)[1]\n",
        "finetuned_output = finetuned_pipe(prompt)[0]['generated_text'].split(prompt, 1)[1]\n",
        "\n",
        "#print(\"\\nOriginal Model Output:\\n\", original_output)\n",
        "print(\"\\nFine-tuned Model Output:\\n\", finetuned_output)"
      ],
      "metadata": {
        "id": "-JVOb7PI-UUk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}